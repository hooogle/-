\documentclass{article}
\title{Auto-Encoding Variational Bayes}
\author{Xinghu Yao}
\date{\today}

\usepackage{geometry}
\geometry{a4paper,scale=0.8}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}


\begin{document}
	\maketitle
	\section{Problem Establishment}
	Using auto-encoding variational Bayes we can perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions.
	
	Let us consider some dataset $\mathbf{X}= \{\mathbf{x}^{(i)}\}_{i=1}^N$ consisting of $N$ i.i.d samples of some continuous or discrete variable $\mathbf{x}$. We assume that the data are generated by some random process, involving an unobserved continuous random variable $\mathbf{z}$. The process consists of two steps: (1) a value $\mathbf{z}^{(i)}$ is generated from some prior distribution $p_{\boldsymbol{\theta}}(\mathbf{z})$. (2) a value $\mathbf{x}^{(i)}$ is generated from some conditional distribution $p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})$.
	
	We can use two networks to train a generative model and the two parts of our networks have the following relationship.
	\begin{equation}
	\mathbf{x}\stackrel{F_{\boldsymbol{\theta}}}{\longrightarrow}\mathbf{(z|x)}\stackrel{G_{\boldsymbol{\theta}}}{\longrightarrow}\hat{\mathbf{x}} 
	\end{equation}
	where $F_{\boldsymbol{\theta}}$ is a network to cover the latent hidden variable $\mathbf{z}$ and $G_{\boldsymbol{\theta}}$ is another network to decode $\mathbf{x}$ using the hidden variable $\mathbf{z}$. Thus, the marginal likelihood of this structure can be written as:
	\begin{align}
	\text{ln}p(\mathbf{x}) &= \text{ln}\int p(\mathbf{x}|\mathbf{z})p(\mathbf{z})\text{d}\mathbf{z}\notag\\
	& = \text{ln}\int \frac{p(\mathbf{x}|\mathbf{z})}{q(\mathbf{\mathbf{z}|\mathbf{x}})}p(\mathbf{z})q(\mathbf{z}|\mathbf{x})\text{d}\mathbf{z}\notag\\
	&= \text{ln}\mathop{\mathds{E}}\limits_{\mathbf{z} \backsim (\mathbf{z}|\mathbf{x})}\left(\frac{p(\mathbf{x}|\mathbf{z})}{q(\mathbf{z}|\mathbf{x})}p(\mathbf{z})\right)\notag\\
	&\geq \mathop{\mathds{E}}\limits_{\mathbf{z}\backsim q(\mathbf{z}|\mathbf{x})}\text{ln}\left(\frac{p(\mathbf{x}|\mathbf{z})}{q(\mathbf{z}|\mathbf{x})}p(\mathbf{z})\right)\tag{Jensen Inequality}\\
	&=\mathop{\mathds{E}}\limits_{\mathbf{z}\backsim q (\mathbf{z}|\mathbf{x})}\text{ln}\frac{p(\mathbf{x}|\mathbf{z})}{q(\mathbf{z}|\mathbf{x})}+\mathop{\mathds{E}}\limits_{\mathbf{z}\backsim q (\mathbf{z}|\mathbf{x})}\text{ln}p(\mathbf{z})\notag\\
	&= \int q(\mathbf{z}|\mathbf{x})\text{ln}p(\mathbf{x}|\mathbf{z})\text{d}\mathbf{z}-\text{KL}\left[q(\mathbf{z}|\mathbf{x})\|p(\mathbf{z})\right]
	\end{align}
	where $q(\mathbf{z|x})$ is the probabilistic encoder, since given a datapoint $\mathbf{x}$ it produce a distribution over the possible values of the code $\mathbf{z}$ from which the datapoint $\mathbf{x}$ could have been generated. In a similar vein we will refer to $p(\mathbf{x|z})$ as a probabilistic decoder, since given a code $\mathbf{z}$ it produce a distribution over the possible corresponding values of $\mathbf{x}$. Thus, we can get the following optimization problem
	\begin{equation}
	\mathop{\text{max}}\limits_{q(\mathbf{z|x})}\mathop{\mathds{E}}\limits_{\mathbf{z}\backsim q(\mathbf{z|x})}\left[\text{ln}p(\mathbf{x|z})\right]-\text{KL}\left[q(\mathbf{z|x})\|p(\mathbf{z})\right]
	\end{equation}
	\section{The reparameterization trick}
	In order to solve our problem we invoke an alternative method for generating samples from $q(\mathbf{z|x})$. The essential parameterization trick is quite simple. Let $z\backsim p(z|x)=\mathcal{N}(\mu,\sigma^2)$. In this case, a valid reparameterization is $z = \mu + \sigma\epsilon$, where $\epsilon$ is an auxiliary noise variable $\epsilon \backsim \mathcal{N}(0,1)$
\end{document}